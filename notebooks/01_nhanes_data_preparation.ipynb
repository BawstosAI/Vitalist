{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NHANES Data Preparation for Organ Aging Analysis\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook implements the **data preparation pipeline** for building organ-specific aging clocks using NHANES (National Health and Nutrition Examination Survey) data.\n",
    "\n",
    "### Scientific Background\n",
    "\n",
    "**Biological aging** is the progressive deterioration of physiological function over time. Different organs may age at different rates, leading to heterogeneous aging patterns across individuals. By analyzing organ-specific biomarkers from NHANES, we can:\n",
    "\n",
    "1. Build **organ clocks** that predict chronological age from biomarkers\n",
    "2. Compute **biological age** for each organ system\n",
    "3. Calculate **age gaps** (biological age - chronological age) to identify organs aging faster or slower than expected\n",
    "\n",
    "### Objectives\n",
    "\n",
    "1. Load NHANES data files (demographics, laboratory, examination)\n",
    "2. Merge tables on participant identifier (SEQN)\n",
    "3. Apply data quality filters and preprocessing\n",
    "4. Handle missing values appropriately\n",
    "5. Encode categorical variables\n",
    "6. Save cleaned data for downstream analysis\n",
    "\n",
    "### Test-Driven Development (TDD) Approach\n",
    "\n",
    "This analysis follows TDD principles:\n",
    "- All functions are tested in `tests/test_*.py`\n",
    "- Comprehensive error handling\n",
    "- Reproducible with random seeds\n",
    "- Data validation at each step\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Project root: C:\\Users\\blanc\\Documents\\Vitalist\n",
      "✓ Source path: C:\\Users\\blanc\\Documents\\Vitalist\\src\n",
      "✓ All imports successful\n"
     ]
    }
   ],
   "source": [
    "# Setup paths - works regardless of kernel working directory\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Get the notebook's directory and project root\n",
    "try:\n",
    "    # When running in Jupyter, __file__ doesn't exist, use a workaround\n",
    "    notebook_path = Path().resolve()\n",
    "    if notebook_path.name == 'notebooks':\n",
    "        project_root = notebook_path.parent\n",
    "    else:\n",
    "        # Assume we're in the notebooks directory\n",
    "        project_root = notebook_path.parent if (notebook_path.parent / 'src').exists() else notebook_path\n",
    "except:\n",
    "    project_root = Path().resolve().parent\n",
    "\n",
    "# Add src to path if not already there\n",
    "src_path = project_root / 'src'\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))\n",
    "\n",
    "print(f\"✓ Project root: {project_root}\")\n",
    "print(f\"✓ Source path: {src_path}\")\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Project modules\n",
    "from organ_aging import config\n",
    "from organ_aging import data_loading\n",
    "from organ_aging import preprocessing\n",
    "\n",
    "print(\"✓ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Configuration\n",
    "\n",
    "We use YAML configuration files to:\n",
    "- Define file paths to NHANES data\n",
    "- Specify organ-specific biomarker panels\n",
    "- Set preprocessing parameters\n",
    "\n",
    "This approach ensures **reproducibility** and **maintainability**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Paths configuration loaded\n",
      "\n",
      "Raw data directory: data/raw\n",
      "Number of NHANES files: 9\n",
      "\n",
      "Files to load:\n",
      "  - demographics: DEMO_J.xpt\n",
      "  - biochemistry: BIOPRO_J.xpt\n",
      "  - blood_pressure: BPX_J.xpt\n",
      "  - body_measures: BMX_J.xpt\n",
      "  - cholesterol: TCHOL_J.xpt\n",
      "  - glucose: GLU_J.xpt\n",
      "  - kidney: ALB_CR_J.xpt\n",
      "  - complete_blood_count: CBC_J.xpt\n",
      "  - glycohemoglobin: GHB_J.xpt\n"
     ]
    }
   ],
   "source": [
    "# Load paths configuration\n",
    "try:\n",
    "    paths_config = config.load_paths_config(str(project_root / \"configs\" / \"paths.yaml\"))\n",
    "    print(\"✓ Paths configuration loaded\")\n",
    "    print(f\"\\nRaw data directory: {paths_config['raw_data_dir']}\")\n",
    "    print(f\"Number of NHANES files: {len(paths_config['nhanes_files'])}\")\n",
    "    print(f\"\\nFiles to load:\")\n",
    "    for table_name, filename in paths_config['nhanes_files'].items():\n",
    "        print(f\"  - {table_name}: {filename}\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"⚠ Warning: {e}\")\n",
    "    print(\"\\nPlease ensure NHANES data files are downloaded and paths.yaml is configured correctly.\")\n",
    "    paths_config = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Organ panels configuration loaded\n",
      "\n",
      "Organ systems defined: 5\n",
      "\n",
      "Organ panels:\n",
      "  - liver: 8 biomarkers\n",
      "  - kidney: 5 biomarkers\n",
      "  - cardio_metabolic: 10 biomarkers\n",
      "  - immune: 6 biomarkers\n",
      "  - hematologic: 8 biomarkers\n"
     ]
    }
   ],
   "source": [
    "# Load organ panels configuration\n",
    "try:\n",
    "    organ_panels = config.load_organ_panels_config(str(project_root / \"configs\" / \"organ_panels.yaml\"))\n",
    "    print(\"✓ Organ panels configuration loaded\")\n",
    "    print(f\"\\nOrgan systems defined: {len([k for k in organ_panels.keys() if k not in ['global_covariates', 'target_variable', 'preprocessing']])}\")\n",
    "    print(f\"\\nOrgan panels:\")\n",
    "    for organ, biomarkers in organ_panels.items():\n",
    "        if organ not in ['global_covariates', 'target_variable', 'preprocessing']:\n",
    "            print(f\"  - {organ}: {len(biomarkers)} biomarkers\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"⚠ Warning: {e}\")\n",
    "    organ_panels = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load NHANES Data Files\n",
    "\n",
    "### NHANES Data Structure\n",
    "\n",
    "NHANES data is typically distributed as separate files:\n",
    "- **Demographics (DEMO)**: Age, sex, race/ethnicity, socioeconomic factors\n",
    "- **Laboratory (LAB)**: Blood chemistry, CBC, metabolic markers\n",
    "- **Examination (BMX)**: Body measurements, blood pressure\n",
    "\n",
    "All files share a common identifier: **SEQN** (sequence number)\n",
    "\n",
    "### Data Loading Strategy\n",
    "\n",
    "We support both:\n",
    "- **XPT files** (SAS Transport format) - official NHANES distribution\n",
    "- **CSV files** - for preprocessed or converted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading NHANES data files...\n",
      "\n",
      "⚠ Error: Data file not found: C:\\Users\\blanc\\Documents\\Vitalist\\data\\raw\\DEMO_J.xpt\n",
      "\n",
      "Please download NHANES data files first.\n"
     ]
    }
   ],
   "source": [
    "# Load NHANES tables\n",
    "if paths_config is not None:\n",
    "    try:\n",
    "        print(\"Loading NHANES data files...\\n\")\n",
    "        tables = data_loading.load_nhanes_tables(paths_config, project_root=project_root)\n",
    "        print(f\"\\n✓ Successfully loaded {len(tables)} tables\")\n",
    "        \n",
    "        # Display summary of each table\n",
    "        print(\"\\nTable summaries:\")\n",
    "        for table_name, df in tables.items():\n",
    "            print(f\"\\n{table_name.upper()}:\")\n",
    "            print(f\"  Shape: {df.shape}\")\n",
    "            print(f\"  Columns: {df.shape[1]}\")\n",
    "            print(f\"  Sample columns: {', '.join(df.columns[:5].tolist())}\")\n",
    "            \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"⚠ Error: {e}\")\n",
    "        print(\"\\nPlease download NHANES data files first.\")\n",
    "        tables = None\n",
    "else:\n",
    "    print(\"⚠ Skipping data loading - configuration not available\")\n",
    "    tables = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Merge Tables on SEQN\n",
    "\n",
    "### Merging Strategy\n",
    "\n",
    "We perform **inner joins** on SEQN to ensure:\n",
    "- Only participants with complete data across all tables are retained\n",
    "- Data integrity is maintained\n",
    "- Downstream analysis has consistent sample sizes\n",
    "\n",
    "### Expected Behavior\n",
    "\n",
    "- Each merge reduces sample size (participants must be present in all files)\n",
    "- Column names are checked for duplicates\n",
    "- SEQN is the primary key linking all tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Skipping merge - tables not loaded\n"
     ]
    }
   ],
   "source": [
    "# Merge all tables on SEQN\n",
    "if tables is not None:\n",
    "    print(\"Merging NHANES tables on SEQN...\\n\")\n",
    "    merged_df = data_loading.merge_nhanes_tables(tables)\n",
    "    print(f\"\\n✓ Tables merged successfully\")\n",
    "    print(f\"\\nFinal merged dataset:\")\n",
    "    print(f\"  Rows: {merged_df.shape[0]:,}\")\n",
    "    print(f\"  Columns: {merged_df.shape[1]}\")\n",
    "    print(f\"  Memory usage: {merged_df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "else:\n",
    "    print(\"⚠ Skipping merge - tables not loaded\")\n",
    "    merged_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample of merged data\n",
    "if merged_df is not None:\n",
    "    print(\"Sample of merged data (first 5 rows, selected columns):\\n\")\n",
    "    sample_cols = ['SEQN', 'RIDAGEYR', 'RIAGENDR'] + [col for col in merged_df.columns if col.startswith('LBX')][:5]\n",
    "    display(merged_df[sample_cols].head())\n",
    "    \n",
    "    # Data types\n",
    "    print(f\"\\nData types:\")\n",
    "    print(merged_df.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Quality Assessment\n",
    "\n",
    "Before preprocessing, we assess data quality:\n",
    "1. **Missing values**: Identify columns/rows with high missingness\n",
    "2. **Duplicates**: Check for duplicate SEQN entries\n",
    "3. **Data ranges**: Verify age and biomarker values are plausible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if merged_df is not None:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"DATA QUALITY ASSESSMENT\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Check for duplicates\n",
    "    n_duplicates = merged_df['SEQN'].duplicated().sum()\n",
    "    print(f\"\\n1. Duplicate SEQN entries: {n_duplicates}\")\n",
    "    if n_duplicates > 0:\n",
    "        print(\"   ⚠ Warning: Duplicate entries detected!\")\n",
    "    \n",
    "    # Missing value analysis\n",
    "    print(f\"\\n2. Missing Values:\")\n",
    "    missing_pct = (merged_df.isnull().sum() / len(merged_df)) * 100\n",
    "    print(f\"   Total cells: {merged_df.shape[0] * merged_df.shape[1]:,}\")\n",
    "    print(f\"   Missing cells: {merged_df.isnull().sum().sum():,}\")\n",
    "    print(f\"   Missing percentage: {missing_pct.mean():.2f}%\")\n",
    "    \n",
    "    # Columns with high missingness\n",
    "    high_missing = missing_pct[missing_pct > 50].sort_values(ascending=False)\n",
    "    if len(high_missing) > 0:\n",
    "        print(f\"\\n   Columns with >50% missing ({len(high_missing)} total):\")\n",
    "        for col, pct in high_missing.head(10).items():\n",
    "            print(f\"     - {col}: {pct:.1f}%\")\n",
    "    \n",
    "    # Age distribution\n",
    "    if 'RIDAGEYR' in merged_df.columns:\n",
    "        print(f\"\\n3. Age Distribution:\")\n",
    "        print(f\"   Min age: {merged_df['RIDAGEYR'].min():.0f} years\")\n",
    "        print(f\"   Max age: {merged_df['RIDAGEYR'].max():.0f} years\")\n",
    "        print(f\"   Mean age: {merged_df['RIDAGEYR'].mean():.1f} ± {merged_df['RIDAGEYR'].std():.1f} years\")\n",
    "        print(f\"   Median age: {merged_df['RIDAGEYR'].median():.0f} years\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missing data pattern\n",
    "if merged_df is not None:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Missing values by column\n",
    "    missing_pct = (merged_df.isnull().sum() / len(merged_df)) * 100\n",
    "    missing_pct = missing_pct[missing_pct > 0].sort_values(ascending=False).head(20)\n",
    "    \n",
    "    axes[0].barh(range(len(missing_pct)), missing_pct.values, color='coral', edgecolor='black')\n",
    "    axes[0].set_yticks(range(len(missing_pct)))\n",
    "    axes[0].set_yticklabels(missing_pct.index, fontsize=8)\n",
    "    axes[0].set_xlabel('Missing Percentage (%)')\n",
    "    axes[0].set_title('Top 20 Columns with Missing Values')\n",
    "    axes[0].axvline(50, color='red', linestyle='--', linewidth=2, label='50% threshold')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Age distribution\n",
    "    if 'RIDAGEYR' in merged_df.columns:\n",
    "        axes[1].hist(merged_df['RIDAGEYR'].dropna(), bins=50, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "        axes[1].axvline(18, color='green', linestyle='--', linewidth=2, label='Min age filter (18)')\n",
    "        axes[1].axvline(80, color='red', linestyle='--', linewidth=2, label='Max age filter (80)')\n",
    "        axes[1].set_xlabel('Age (years)')\n",
    "        axes[1].set_ylabel('Count')\n",
    "        axes[1].set_title('Age Distribution Before Filtering')\n",
    "        axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Preprocessing\n",
    "\n",
    "### Preprocessing Pipeline\n",
    "\n",
    "1. **Age Filtering**: Restrict to adults aged 18-80 years\n",
    "   - Rationale: Biological aging patterns differ in children/adolescents\n",
    "   - Upper limit: Reduce survivor bias and ensure data quality\n",
    "\n",
    "2. **Missing Value Handling**:\n",
    "   - Drop columns with >50% missing data (unreliable)\n",
    "   - Impute remaining missing values using median (robust to outliers)\n",
    "\n",
    "3. **Column Standardization**: Ensure consistent naming\n",
    "\n",
    "### Scientific Considerations\n",
    "\n",
    "- **Missing data mechanism**: NHANES uses complex survey design; some missingness is by design (e.g., subsampling for expensive assays)\n",
    "- **Imputation strategy**: Median is preferred over mean due to skewed biomarker distributions\n",
    "- **Age range**: Standard in aging research to focus on adult lifespan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Skipping age filtering\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Filter by age (18-80 years)\n",
    "if merged_df is not None and 'RIDAGEYR' in merged_df.columns:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"STEP 1: AGE FILTERING\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    min_age = organ_panels.get('preprocessing', {}).get('min_age', 18)\n",
    "    max_age = organ_panels.get('preprocessing', {}).get('max_age', 80)\n",
    "    \n",
    "    df_clean = preprocessing.filter_by_age(\n",
    "        merged_df,\n",
    "        min_age=min_age,\n",
    "        max_age=max_age,\n",
    "        age_col='RIDAGEYR'\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n✓ Age filtering complete\")\n",
    "    print(f\"  Age range: {min_age}-{max_age} years\")\n",
    "    print(f\"  Retention rate: {100 * len(df_clean) / len(merged_df):.1f}%\")\n",
    "else:\n",
    "    print(\"⚠ Skipping age filtering\")\n",
    "    df_clean = merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Handle missing values\n",
    "if df_clean is not None:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"STEP 2: MISSING VALUE HANDLING\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    missing_threshold = organ_panels.get('preprocessing', {}).get('missing_threshold', 0.5)\n",
    "    imputation_strategy = organ_panels.get('preprocessing', {}).get('imputation_strategy', 'median')\n",
    "    \n",
    "    df_clean = preprocessing.handle_missing_values(\n",
    "        df_clean,\n",
    "        missing_threshold=missing_threshold,\n",
    "        strategy=imputation_strategy,\n",
    "        numeric_only=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n✓ Missing value handling complete\")\n",
    "    print(f\"  Threshold: {missing_threshold*100}%\")\n",
    "    print(f\"  Imputation: {imputation_strategy}\")\n",
    "    print(f\"  Remaining missing values: {df_clean.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Standardize column names\n",
    "if df_clean is not None:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"STEP 3: COLUMN STANDARDIZATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    df_clean = preprocessing.standardize_column_names(df_clean)\n",
    "    \n",
    "    print(f\"\\n✓ Column names standardized\")\n",
    "    print(f\"  All columns uppercase: {all(col.isupper() for col in df_clean.columns)}\")\n",
    "    print(f\"  Sample columns: {', '.join(df_clean.columns[:10].tolist())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Categorical Variable Encoding\n",
    "\n",
    "### Encoding Strategy\n",
    "\n",
    "Categorical variables (e.g., sex, race/ethnicity) must be encoded for machine learning:\n",
    "- **One-hot encoding**: Create binary dummy variables\n",
    "- **Drop first**: Avoid multicollinearity (drop one dummy as reference)\n",
    "\n",
    "### NHANES Categorical Variables\n",
    "\n",
    "- **RIAGENDR**: Gender (1=Male, 2=Female)\n",
    "- **RIDRETH1**: Race/Ethnicity (1=Mexican American, 2=Other Hispanic, 3=Non-Hispanic White, 4=Non-Hispanic Black, 5=Other)\n",
    "- Other sociodemographic factors if included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify and encode categorical variables\n",
    "if df_clean is not None:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"CATEGORICAL VARIABLE ENCODING\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Define categorical columns (NHANES specific)\n",
    "    categorical_cols = ['RIAGENDR', 'RIDRETH1']\n",
    "    categorical_cols = [col for col in categorical_cols if col in df_clean.columns]\n",
    "    \n",
    "    if categorical_cols:\n",
    "        print(f\"\\nCategorical variables to encode:\")\n",
    "        for col in categorical_cols:\n",
    "            print(f\"  - {col}: {df_clean[col].nunique()} unique values\")\n",
    "        \n",
    "        df_clean = preprocessing.encode_categorical_variables(\n",
    "            df_clean,\n",
    "            categorical_cols=categorical_cols,\n",
    "            drop_first=True\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n✓ Categorical encoding complete\")\n",
    "    else:\n",
    "        print(\"\\nNo categorical columns found for encoding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Data Summary\n",
    "\n",
    "Review the cleaned dataset before saving:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_clean is not None:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"FINAL CLEANED DATASET SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(f\"\\nDataset Shape:\")\n",
    "    print(f\"  Rows (participants): {df_clean.shape[0]:,}\")\n",
    "    print(f\"  Columns (features): {df_clean.shape[1]:,}\")\n",
    "    \n",
    "    print(f\"\\nData Quality:\")\n",
    "    print(f\"  Missing values: {df_clean.isnull().sum().sum():,}\")\n",
    "    print(f\"  Duplicate rows: {df_clean.duplicated().sum()}\")\n",
    "    \n",
    "    print(f\"\\nAge Statistics:\")\n",
    "    if 'RIDAGEYR' in df_clean.columns:\n",
    "        print(f\"  Range: {df_clean['RIDAGEYR'].min():.0f} - {df_clean['RIDAGEYR'].max():.0f} years\")\n",
    "        print(f\"  Mean ± SD: {df_clean['RIDAGEYR'].mean():.1f} ± {df_clean['RIDAGEYR'].std():.1f} years\")\n",
    "        print(f\"  Median [IQR]: {df_clean['RIDAGEYR'].median():.0f} [{df_clean['RIDAGEYR'].quantile(0.25):.0f}-{df_clean['RIDAGEYR'].quantile(0.75):.0f}] years\")\n",
    "    \n",
    "    print(f\"\\nMemory Usage:\")\n",
    "    memory_mb = df_clean.memory_usage(deep=True).sum() / 1024**2\n",
    "    print(f\"  Total: {memory_mb:.2f} MB\")\n",
    "    print(f\"  Per participant: {memory_mb * 1024 / len(df_clean):.2f} KB\")\n",
    "    \n",
    "    # Check for key organ biomarkers\n",
    "    print(f\"\\nKey Biomarkers Present:\")\n",
    "    key_markers = {\n",
    "        'Liver': ['LBXSATSI', 'LBXSASSI', 'LBXSGTSI'],\n",
    "        'Kidney': ['LBXSCR', 'LBXSUA'],\n",
    "        'Cardio': ['BPXSY1', 'LBXTC', 'LBXGLU'],\n",
    "        'Hematologic': ['LBXHGB', 'LBXWBCSI', 'LBXPLTSI']\n",
    "    }\n",
    "    \n",
    "    for system, markers in key_markers.items():\n",
    "        present = sum(1 for m in markers if m in df_clean.columns)\n",
    "        print(f\"  {system}: {present}/{len(markers)} markers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize final age distribution\n",
    "if df_clean is not None and 'RIDAGEYR' in df_clean.columns:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Age histogram\n",
    "    axes[0].hist(df_clean['RIDAGEYR'], bins=30, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "    axes[0].axvline(df_clean['RIDAGEYR'].mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {df_clean[\"RIDAGEYR\"].mean():.1f}')\n",
    "    axes[0].axvline(df_clean['RIDAGEYR'].median(), color='orange', linestyle='--', linewidth=2, label=f'Median: {df_clean[\"RIDAGEYR\"].median():.1f}')\n",
    "    axes[0].set_xlabel('Age (years)', fontsize=12)\n",
    "    axes[0].set_ylabel('Count', fontsize=12)\n",
    "    axes[0].set_title('Age Distribution (Cleaned Data)', fontsize=14, fontweight='bold')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Age by gender (if available)\n",
    "    gender_cols = [col for col in df_clean.columns if col.startswith('RIAGENDR')]\n",
    "    if gender_cols:\n",
    "        # Reconstruct gender from dummy variables\n",
    "        if 'RIAGENDR' in df_clean.columns:\n",
    "            gender_data = df_clean['RIAGENDR']\n",
    "        else:\n",
    "            # If one-hot encoded\n",
    "            gender_data = df_clean[gender_cols[0]]\n",
    "        \n",
    "        axes[1].boxplot([df_clean['RIDAGEYR'].values], labels=['All'], vert=True, patch_artist=True)\n",
    "        axes[1].set_ylabel('Age (years)', fontsize=12)\n",
    "        axes[1].set_title('Age Distribution Summary', fontsize=14, fontweight='bold')\n",
    "        axes[1].grid(axis='y', alpha=0.3)\n",
    "    else:\n",
    "        # Just show box plot\n",
    "        axes[1].boxplot([df_clean['RIDAGEYR'].values], labels=['All Participants'], patch_artist=True)\n",
    "        axes[1].set_ylabel('Age (years)', fontsize=12)\n",
    "        axes[1].set_title('Age Distribution Summary', fontsize=14, fontweight='bold')\n",
    "        axes[1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Cleaned Data\n",
    "\n",
    "### Output Format: Parquet\n",
    "\n",
    "We save the cleaned data in **Parquet** format because:\n",
    "- **Efficient compression**: Smaller file size than CSV\n",
    "- **Fast I/O**: Faster to read/write than CSV\n",
    "- **Type preservation**: Maintains data types (no need to re-specify)\n",
    "- **Column-oriented**: Optimized for analytical queries\n",
    "\n",
    "### Output Location\n",
    "\n",
    "`data/interim/nhanes_clean.parquet`\n",
    "\n",
    "This intermediate file will be used by subsequent notebooks for:\n",
    "- Feature engineering\n",
    "- Model training\n",
    "- Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ No cleaned data to save\n"
     ]
    }
   ],
   "source": [
    "# Save cleaned data\n",
    "if df_clean is not None:\n",
    "    output_dir = project_root / \"data\" / \"interim\"\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    output_path = output_dir / \"nhanes_clean.parquet\"\n",
    "    \n",
    "    print(\"Saving cleaned data...\")\n",
    "    df_clean.to_parquet(output_path, index=False, compression='snappy')\n",
    "    \n",
    "    # Verify save\n",
    "    file_size_mb = output_path.stat().st_size / 1024**2\n",
    "    \n",
    "    print(f\"\\n✓ Data saved successfully\")\n",
    "    print(f\"  Path: {output_path}\")\n",
    "    print(f\"  File size: {file_size_mb:.2f} MB\")\n",
    "    print(f\"  Format: Parquet (Snappy compression)\")\n",
    "    \n",
    "    # Test reload\n",
    "    print(f\"\\nVerifying saved data...\")\n",
    "    df_test = pd.read_parquet(output_path)\n",
    "    print(f\"  Reload successful: {df_test.shape}\")\n",
    "    print(f\"  Data integrity: {df_test.shape == df_clean.shape}\")\n",
    "else:\n",
    "    print(\"⚠ No cleaned data to save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Next Steps\n",
    "\n",
    "### What We Accomplished\n",
    "\n",
    "✓ Loaded NHANES data from multiple source files\n",
    "✓ Merged tables on participant identifier (SEQN)\n",
    "✓ Filtered data to adult age range (18-80 years)\n",
    "✓ Handled missing values systematically\n",
    "✓ Encoded categorical variables\n",
    "✓ Saved cleaned data for downstream analysis\n",
    "\n",
    "### Data Quality Metrics\n",
    "\n",
    "- **Participants**: N individuals aged 18-80\n",
    "- **Variables**: Complete biomarker panel + demographics\n",
    "- **Missing data**: Handled via column dropping + imputation\n",
    "- **Format**: Parquet (efficient, type-safe)\n",
    "\n",
    "### Next Notebook: Feature Engineering\n",
    "\n",
    "In `02_feature_engineering_organs.ipynb`, we will:\n",
    "1. Build organ-specific feature matrices\n",
    "2. Create train/validation/test splits\n",
    "3. Apply feature scaling\n",
    "4. Prepare data for model training\n",
    "\n",
    "---\n",
    "\n",
    "**TDD Note**: All preprocessing functions used in this notebook are tested in `tests/test_preprocessing.py` and `tests/test_data_loading.py`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dopamine)",
   "language": "python",
   "name": "dopamine"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
